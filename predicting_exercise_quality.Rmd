---
title: "Predicting how well people exercise"
author: "Meenakshi Parameshwaran"
date: "19 February 2016"
output: html_document
---
```{r options, echo = F}
library(knitr)
opts_chunk$set(message = F, warnings = F, tidy = T)
```

## Executive Summary


## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify *how well they do it*.  

The goal of my project is to predict exercise quality - specifically the manner in which 6 people carried out barbell lifts. I use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from their website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

## Methods

In this project, I used the `caret` package to implement machine learning algorithms.

First I load the training and testing datasets. The data for this project kindly comes from this source: http://groupware.les.inf.puc-rio.br/har. 

```{r load data, tidy = T, cache = T}
# clear any objects from the working space
rm(list = ls())

# set the working directory
setwd("~/GitHub/Practical_Machine_Learning_Project")

# load required packages
library(caret)
library(knitr)
library(scales)

# get the training data
if(!file.exists("pml-training.csv")){download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv", method = "curl")}

# get the test data
if(!file.exists("pml-testing.csv")){download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv", method = "curl")}

# read in the datasets
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

# look at the dimensions of the datasets
dim(training)
dim(testing)

# look at a summary of the training data to check for missing data (output not shown)
# summary(training)

# look at the outcome variable
table(training$classe)
```

The variable I predict is called `classe` and has five categories: A, B, C, D, and E.

#### Pre-processing

I carried out some preprocessing of the data, including converting some factor variables back to integers and removing predictors with near zero variance.

```{r preprocessing, tidy = T, cache = T}

# read in the data
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

# check structure of variables - output not shown
# str(training)

# separate the outcome variable
classe <- as.data.frame(training[, 160])

# identify the variables I want to keep as factors
training_factors <- training[, c(2,5,6)]

# convert all variables to numeric except user_name, ctvd_timestamp, new_window and classe
training_conv <- as.data.frame(lapply(training[, -c(2,5,6,160)], function(x) {as.numeric(as.character(x))} ))

# remove numeric variables with near zero variance
training_nzv <- nearZeroVar(training_conv, saveMetrics = T)
training_keepvars <- subset(training_nzv, nzv == F) # 121 vars to keep
training_keepvars <- row.names(training_keepvars)
training_conv_nzv <- subset(training_conv, select = training_keepvars)

# check for missing data - how many complete cases?
sum(complete.cases(training_conv_nzv))

# impute missing data
imputeObj <- preProcess(x = training_conv_nzv, method = "knnImpute")
training_imputed <- predict(object = imputeObj, newdata = training_conv_nzv)

# bring the dataset back together                   
training_processed <- as.data.frame(cbind(training_imputed, classe))
names(training_processed)[122] <- "classe"

### repeat the processing for the testing dataset

# separate the outcome variable
classe <- testing[, 160]

# identify the variables I want to keep as factors
testing_factors <- testing[, c(2,5,6)]

# convert all variables to numeric except user_name, ctvd_timestamp, new_window and classe
testing_conv <- as.data.frame(lapply(testing[, -c(2,5,6,160)], function(x) {as.numeric(as.character(x))} ))

# remove the same variables because of near zero variance
testing_conv_nzv <- subset(testing_conv, select = training_keepvars)

# check for missing data - how many complete cases?
sum(complete.cases(testing_conv_nzv))

# impute missing data
testing_imputed <- predict(object = imputeObj, newdata = testing_conv_nzv)

# bring the dataset back together                    
testing_processed <- as.data.frame(cbind(testing_imputed, classe))
names(testing_processed)[122] <- "problem_id"
    
# remove all other objects except the processed datasets
rm(list=setdiff(ls(), c("training_processed", "testing_processed")))

# check dimensions of processed datasets
dim(training_processed)
dim(testing_processed)

# rename the datasets
testing <- testing_processed
training <- training_processed
rm(testing_processed, training_processed)
```

#### Splitting the training dataset

Below I split the training dataset into a testing dataset to allow me to test my predictive model before applying it to the provided testing dataset. To avoid confusion, I call my own testing dataset `mytesting`.

```{r split the training dataset, tidy = T}
set.seed(4568)
inTrain <- createDataPartition(y = training$classe,
                              p=0.7, list=FALSE)
mytraining <- training[inTrain,]
mytesting <- training[-inTrain,]
```

#### Modelling strategy and justification of choices
I built my model with the aims of the "best" maching learning method (slide 8 from the "Relative importance of steps" lecture): 

- interpretable
- simple
- accurate
- fast (to train and to test)
- scalable

The final slide of the "Random Forests" lecture noted that:

> Random forests are usually one of the two top performing algorithms along with boosting in prediction contests.

Based on this, I decided to use **random forest** `(method = "rf")` and **boosting** `(method = "gbm")` algorithms in my prediction model. Additionally, I tested out **naive Bayes** and **linear discriminant analysis** classifiers.

Because of a large number of missing data points, I used the `preProcess` function to impute the missing data. I used k-nearest neighbours with default settings (5 neighbours, mean column values) to impute the data.

There were a large number of potential predictors in the dataset, some of which contained quite a few NAs. The large number of variables made it difficult to carry out exploratory plots, so I pre-processed the data using **principal component analysis (PCA)** to reduce the number of dimensions and the amount of noise, whilst maximising the variance retained. I used the threshold variance default of 0.95.

I carried out the PCA on the data whilst training my four models, using a 5-fold cross-validation each time. I chose this approach to reduce bias, although variance goes up with more folds. I used k-folds rather than bootstrapping to allow faster computation (using parallel processing) and followed methods for implementing parallel processing from the [DSS Community Site](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md)

```{r fit models, tidy = T, cache = T}
# set up parallel processing in caret - code taken from DSS Community site
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(13489) # set the seed at each interation, and keep them across model fits 
seeds <- vector(mode = "list", length = 26)
for(i in 1:25) seeds[[i]] <- sample.int(1000, 22)
#for the last model:
seeds[[26]] <- sample.int(1000, 1)

# put the trainControl options into an object for ease - make sure allowParallel is set to true
myctrl <- trainControl(method = "cv",
                       number = 5,
                       seeds = seeds,
                       allowParallel = TRUE,
                       savePredictions="final",
                       index=createResample(mytraining$classe, 25),
                       classProbs=TRUE,
                       preProcOptions = list(preProcess="pca")
)

# use caretList to fit a series of models with the same trainControl options
library(caretEnsemble)

# take a sample of the data to speed up model fitting
set.seed(100)
mytraining_sample <- mytraining[sample(nrow(mytraining), 1000), ]
    
model_list <- caretList(
  x = mytraining_sample[, -122],
  y = mytraining_sample[, 122],
  trControl = myctrl,
  tuneList = list(
    rf = caretModelSpec(method="rf", prox = T),
    gbm = caretModelSpec(method="gbm", verbose = F),
    nb = caretModelSpec(method = "nb"),
    lda = caretModelSpec(method = "lda"))
  )

# create the stacked model
glm_ensemble <- caretStack(
  model_list,
  method="glm",
  metric="ROC",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE)
)

# stop parallel processing
stopCluster(cluster)
```

The accuracy of the four predictive models was as follows:

```{r accuracy tables, results = "asis", eval = F, echo = F}

mymodels <- list(modFitRf,modFitBoost,modFitNb,modFitLda)
mytable_labels <- list()
mytable_values <- list()

for (i in 1:length(mymodels)) {
    mytable_labels[i] <- mymodels[[i]]$modelInfo$label
    mytable_values[i] <- round(max(mymodels[[i]]$results$Accuracy),3)
}

mytable <- cbind(mytable_labels, mytable_values)
library(knitr)
kable(mytable, digits = 3, type = "html", row.names = NA, col.names = c("Classifier", "Accuracy") )
```

I compared the accuracy of the four models and decided to select the linear discriminant analysis modern as my final model choice. I think applied the prediction from the LDA model on `mytesting` dataset.


```{r prediction mytesting, tidy = T, cache = T}

# predict using the ensemble on mytesting dataframe
set.seed(1234)
mytesting_preds <- predict(glm_ensemble, newdata=mytesting, type="raw")
myconfusionmatrix <- confusionMatrix(mytesting_preds, mytesting_complete$classe)
```


## Predicting on the testing dataset

Finally I applied my combined prediction model to the provided real testing dataset. This gave the following results:

```{r final testing, tidy = T}
predfinal <- predict(glm_ensemble, testing[,-122])
extractPrediction(predfinal)
```
